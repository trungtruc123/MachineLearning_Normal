{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 0 2 1 0 2 0 2 0]\n",
      "loss if without regularization: 6.443917350191055\n",
      "loss if with regularization:  7.554177384246895\n",
      "<function <lambda> at 0x000001D41DF4F840>\n",
      "norm:  3.1612752429049813e-09\n",
      "time navie:  20.34653663635254\n",
      "time navie:  0.5651655197143555\n",
      "loss difference:  4.547473508864641e-13\n",
      "grad difference:  2.698408027214262e-14\n"
     ]
    }
   ],
   "source": [
    "#Tính hàm mất mát (loss) và đạo hàm (grad) của nó bằng cách naive\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(2)\n",
    "# sample data small\n",
    "N,C,d = 10,3, 5 # số điểm dữ liệu, 3 class, dimension =5\n",
    "reg =0.1 # regularization\n",
    "W = np.random.randn(d,C)\n",
    "X = np.random.randn(d,N)\n",
    "y = np.random.randint(C, size =N)\n",
    "print(y)\n",
    "\n",
    "# naive way to calculate loss and grad\n",
    "def svm_loss_naive(W,X,y,reg):\n",
    "    d,C = W.shape # W(d,C)\n",
    "    _,N = X.shape # X (N)\n",
    "    # naive loss and grad\n",
    "    loss =0 # loss = 1/N *sum(max(margin)) + reg/2* sum(w*w)\n",
    "    dw = np.zeros_like(W) # đạo hàm : nếu margin<0 =>đạo hàm =0;còn nếu >0 => đạo hàm sẽ = -Xn nếu đạo hàm theo Wyn, còn đạo hàm sẽ bằng Xn nếu đạo hàm theo Wj\n",
    "    for n in range(N):\n",
    "        xn = X[:,n] # 1 điểm dữ liệu\n",
    "        score = W.T.dot(xn) # zn\n",
    "        for j in range(C):\n",
    "            if( j == y[n]):\n",
    "                continue\n",
    "            margin = 1 - score[y[n]] + score[j] # margin = 1 - (Wyn).T* xn + (Wj).T*xn\n",
    "            if margin >0:\n",
    "                loss += margin\n",
    "                dw[:,y[n]] -= xn\n",
    "                dw[:,j]    += xn\n",
    "    # loss function\n",
    "    loss /=N\n",
    "    loss += 0.5*reg*np.sum(W*W) # + regularization\n",
    "    # grad\n",
    "    dw /=N\n",
    "    dw += reg*W\n",
    "    return loss, dw\n",
    "print('loss if without regularization:', svm_loss_naive(W,X,y,0)[0])\n",
    "print('loss if with regularization: ', svm_loss_naive(W,X,y,0.1)[0])\n",
    "\n",
    "# check xem grad có đúng không?\n",
    "f = lambda W : svm_loss_naive(W,X,y,0.1)[0]\n",
    "print(f)\n",
    "\n",
    "def numerical_grad(W,f):\n",
    "    eps = 1e-6\n",
    "    g = np.zeros_like(W)\n",
    "    # flatening variable -> 1d. Then we need \n",
    "    # only one for loop\n",
    "    W_flattened = W.flatten() # làm phẳng ma trận thành 1 vector\n",
    "    g_flattened = np.zeros_like(W_flattened)\n",
    "    for i in range(W.size):\n",
    "        W_p = W_flattened.copy()\n",
    "        W_n = W_flattened.copy()\n",
    "        W_p[i]  += eps\n",
    "        W_n[i]  -=eps\n",
    "        # back to shape of W \n",
    "        W_p = W_p.reshape(W.shape)\n",
    "        W_n = W_n.reshape(W.shape)\n",
    "        g_flattened[i] = (f(W_p)-f(W_n))/(2*eps)\n",
    "    # convert back to original shape\n",
    "    return g_flattened.reshape(W.shape)\n",
    "\n",
    "g1 = svm_loss_naive(W,X,y,0.1)[1]\n",
    "g2 = numerical_grad(W,f)\n",
    "print('norm: ', np.linalg.norm(g1-g2))\n",
    "# norm should very small \n",
    "\n",
    "# Tính hàm mất mát và đạo hàm của nó bằng cách vectorized ( giảm hơn 140 lần đạo hàm bằng vòng for)\n",
    "def svm_loss_vectorized(W,X,y,reg):\n",
    "    d, C = W.shape\n",
    "    _, N = X.shape\n",
    "    loss =0\n",
    "    dw = np.zeros_like(W)\n",
    "    \n",
    "    Z = W.T.dot(X)\n",
    "    correct_class_score = np.choose(y,Z).reshape(N,1).T\n",
    "    margin = np.maximum(0,Z - correct_class_score +1)\n",
    "    margin[y, np.arange(margin.shape[1])] = 0\n",
    "    loss = np.sum( margin, axis =(0,1))\n",
    "    loss /=N\n",
    "    loss += 0.5*reg*np.sum(W*W)\n",
    "    \n",
    "    F = (margin > 0).astype(int)\n",
    "    F [y, np.arange(F.shape[1])] = np.sum(-F, axis =0)\n",
    "    dw = X.dot(F.T)/N + reg*W\n",
    "    return loss, dw\n",
    "\n",
    "\n",
    "N,C, d = 49000, 10, 3073\n",
    "W = np.random.randn(d,C)\n",
    "X = np.random.randn(d,N)\n",
    "y = np.random.randint(C, size =N)\n",
    "import time\n",
    "t1 = time.time()\n",
    "loss1, dw1 = svm_loss_naive(W,X,y,0.1)\n",
    "t2 = time.time()\n",
    "print('time navie: ',(t2-t1))\n",
    "t3 = time.time()\n",
    "loss2, dw2 = svm_loss_vectorized(W,X,y,0.1)\n",
    "t4 = time.time()\n",
    "print('time navie: ',(t4-t3))\n",
    "# độ sai lệch\n",
    "print('loss difference: ',loss2 -loss1)\n",
    "print('grad difference: ', np.linalg.norm(dw2 -dw1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 0 loss:  1774.132040076219\n",
      "it: 100 loss:  250.21889948971824\n",
      "it: 200 loss:  61.516449009981734\n",
      "it: 300 loss:  40.79514608556997\n",
      "it: 400 loss:  44.65437011818748\n"
     ]
    }
   ],
   "source": [
    "# Gradient Descent cho Multi-class SVM\n",
    "\n",
    "def multiclass_svm_GD(X,y,W_init, reg,batch_size =100, eta =0.1, iters =1000, print_every =100):\n",
    "    W = W_init\n",
    "    loss_history = np.zeros((iters))\n",
    "    for it in range(iters):\n",
    "        # randomly pick a batch of X\n",
    "        idx = np.random.choice(X.shape[1], batch_size)\n",
    "        X_batch = X[:,idx]\n",
    "        y_batch =y[idx]\n",
    "        \n",
    "        loss_history[it], dw = svm_loss_vectorized(W,X_batch, y_batch, reg)\n",
    "        \n",
    "        W = W - eta*dw\n",
    "        if it% print_every ==0:\n",
    "            print('it:',it,'loss: ',loss_history[it])\n",
    "    return W,loss_history\n",
    "N,C,d = 49000, 10, 3073\n",
    "W = np.random.randn(d,C)\n",
    "X = np.random.randn(d,N)\n",
    "y = np.random.randint(C, size =N)\n",
    "W, loss_history = multiclass_svm_GD(X,y,W,0.1)\n",
    "plt.plot(loss_history)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
